{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinakeshvari/NLP_Exercise_ShokrzadCourse/blob/main/Assignment04_DS04_S04_TextGeneration_RNNs_RezaShokrzad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character-Level Text Generation with RNN, LSTM, and GRU\n",
        "In this notebook, we will build and train three character-level text generation models on a collection of poems by William Blake. The text is sourced from Poems of William Blake (Project Gutenberg eBook #574)​ `GUTENBERG.ORG`\n",
        ", which includes Songs of Innocence and of Experience. We will implement:\n",
        "1. **A Vanilla RNN model** (simple recurrent neural network),\n",
        "2. **An LSTM model** (Long Short-Term Memory network),\n",
        "3. **A GRU model** (Gated Recurrent Unit network).\n",
        "\n",
        "Each model will be trained to predict the next character in the sequence given the previous characters. We use the same training configuration (sequence length, epochs, optimizer, etc.) for all models to compare their training performance fairly. Finally, we'll visualize the training loss curves for all three models on a single plot to see how they differ.\n",
        "\n"
      ],
      "metadata": {
        "id": "jxdH5qaZXrFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Preprocessing\n",
        "First, we need to download the corpus (a set of Blake's poems) from Project Gutenberg and preprocess it for training. The preprocessing involves:\n",
        "* Downloading the text and decoding it to plain UTF-8.\n",
        "* Removing Project Gutenberg license headers/footers.\n",
        "* Converting the text into a sequence of character indices for model training.\n",
        "\n",
        "The text will remain in its original casing and format (including punctuation and newlines) to preserve the structure of the poems. Newlines are important as they signify line breaks in poetry, and the model will learn to include them appropriately.\n",
        "\n",
        "\n",
        "Let's download the data and preprocess it:"
      ],
      "metadata": {
        "id": "tdSRkPdLYBbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the Project Gutenberg text for \"Poems of William Blake\"\n",
        "url = \"https://www.gutenberg.org/ebooks/574.txt.utf-8\"\n",
        "try:\n",
        "    response = _______.get(url)\n",
        "    text = _______.text  # raw text\n",
        "except Exception as e:\n",
        "    text = None\n",
        "    print(\"Download failed. If running offline, please manually download the text and place it in the working directory.\")\n",
        "\n",
        "if _______ is None:\n",
        "    # Fallback: try to read from a local file if the download didn't happen\n",
        "    try:\n",
        "        with open(\"blake_poems.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f._______()\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(\"Text data not available. Please provide the corpus text.\")\n",
        "\n",
        "# Remove Gutenberg header and footer\n",
        "start_marker = \"*** START OF\"\n",
        "end_marker   = \"*** END OF\"\n",
        "start_idx = text._______(start_marker)\n",
        "if start_idx != -1:\n",
        "    # Skip ahead to the end of the start marker line\n",
        "    text = text[_______.find(\"\\n\", start_idx) + 1:]\n",
        "_______ = text.find(end_marker)\n",
        "if end_idx != -1:\n",
        "    text = text[:end_idx]\n",
        "text = text._______()  # remove extra whitespace\n",
        "\n",
        "# Display basic info about the text\n",
        "print(f\"Text length: {_______(text)} characters\")\n",
        "print(\"First 200 characters of text:\\n\")\n",
        "print(text[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempt to fetch the text via HTTP. If that fails (e.g., running in an offline environment), the code expects a local file blake_poems.txt with the content. After loading, we strip out the Project Gutenberg license text by removing everything before the \"START OF...\" marker and after the \"END OF...\" marker. Finally, we print the length of the text and a sample of the first 200 characters to verify the content."
      ],
      "metadata": {
        "id": "mFwgML0EYVpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preparing the Dataset for Character-Level Modeling\n",
        "For character-level language modeling, we need to map each character in the text to an integer index (and vice versa). We'll create a vocabulary of all unique characters present in the text. This will include letters, punctuation, whitespace, and newline characters.\n",
        "\n",
        "Then we will convert the entire text into a sequence of these integer indices. This numeric sequence is what we will feed into our models."
      ],
      "metadata": {
        "id": "_aswp81lYX7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary and mappings\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = _______(chars)\n",
        "char_to_idx = {ch: i for i, ch in _______(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx._______()}\n",
        "print(f\"Vocabulary size: {vocab_size} characters\")\n",
        "\n",
        "# Convert text to sequence of indices\n",
        "data_indices = [char_to_idx[_______] for ch in text]\n",
        "\n",
        "# Print a mapping example\n",
        "print(\"Sample char-to-index mapping:\")\n",
        "for _______ in list(char_to_idx)[:10]:\n",
        "    print(f\"  '{ch}' -> {_______[ch]}\")\n"
      ],
      "metadata": {
        "id": "T0tuCxY3UKkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will output the vocabulary size and a few sample mappings (for example, it might show something like `a' -> 1`, `'b' -> 2`, etc., depending on the characters in the text). With the data in index form, we next prepare training sequences. We will use a fixed sequence length (for example, 100 characters). Each training sample will consist of an input sequence of 100 characters and a target sequence of the same length, which is the input sequence shifted by one character into the future. In other words, for an input sequence representing characters `t, t+1, ..., t+99`, the target will be the characters `t+1, t+2, ..., t+100`. This way, at each position the model learns to predict the next character. We also decide on a batch size or how to iterate over the text. For simplicity, we will use a batch size of 1 (process one sequence at a time) and iterate sequentially through the text in chunks of 100 characters. This is a form of truncated backpropagation through time, where we reset the model's hidden state at the start of each chunk to manage sequence length and memory. (We will ignore any trailing part of the text that doesn't fit into a full sequence for simplicity.)"
      ],
      "metadata": {
        "id": "JHWC_IS3Ye-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Architecture: RNN vs. LSTM vs. GRU\n",
        "\n",
        "Before coding the models, let's briefly discuss their architectures and differences:\n",
        "* **Vanilla RNN:** This is the basic recurrent neural network unit. It takes the previous hidden state and the current input character (after embedding) to produce a new hidden state. It typically uses a simple activation function like Tanh. Vanilla RNNs are simple and fast, but they suffer from the short-term\n",
        "\n",
        "\n",
        "> memory problem: they struggle to retain information over long sequences due to the vanishing gradient problem​\n",
        "\n",
        "\n",
        "*In other words, as an RNN processes more time steps, the influence of early characters on later outputs diminishes exponentially. The advantage of a simple RNN is that it has fewer parameters and thus is computationally cheaper and faster to train, but it may fail to capture long-range dependencies in text.*\n",
        "\n",
        "* **LSTM (Long Short-Term Memory):** LSTMs are an extension of RNNs designed to address the vanishing gradient issue. They introduce an internal cell state that runs through the sequence with linear interactions, and a set of gating mechanisms (input, forget, and output gates) to control the flow of information. The input gate controls how much new information to let into the cell state, the forget gate controls what information to throw away from the cell, and the output gate controls how much of the cell state to expose to the output. By using these gates, LSTMs can learn what to keep in memory and what to forget, enabling them to capture long-term dependencies much better than vanilla RNNs. LSTMs, however, have more parameters (due to the gates and cell state), which can make them slower to train compared to a simple RNN.\n",
        "\n",
        "\n",
        "* **GRU (Gated Recurrent Unit):** GRUs are a slightly simpler gating mechanism introduced as a variation of LSTMs. A GRU combines the cell state and hidden state into one and uses two gates: an update gate and a reset gate. This simplification often makes GRUs computationally lighter while still retaining the ability to handle longer-term dependencies more effectively than vanilla RNNs. The update gate in a GRU acts similarly to the combination of input and forget gates in an LSTM, and the reset gate determines how much of the past information to forget. GRUs often achieve similar performance to LSTMs on sequence tasks but with a bit less computational cost. In summary, GRUs can be seen as a middle ground between simple RNNs and LSTMs - they have gating for memory control like LSTMs but a simpler architecture (fewer gates), which sometimes helps in faster training.\n",
        "\n",
        "\n",
        "In our implementation, all three models will share a common structure around the recurrent unit:\n",
        "\n",
        "- An embedding layer that converts character indices to dense vector representations. (This helps the model learn a good feature space for characters, though for one-hot characters it's not strictly necessary; it can still be beneficial.)\n",
        "- A recurrent layer (RNN, LSTM, or GRU) that processes the sequence of embeddings.\n",
        "- A final fully-connected (Linear) layer that maps the recurrent layer's output at each time step to a probability distribution over the next character (essentially logits for each character in the vocabulary).\n",
        "\n",
        "\n",
        "Now let's implement the three models in PyTorch. We'll define three `nn.Module` classes: one for a Vanilla RNN, one for LSTM, and one for GRU. Each will take as input the vocabulary size, embedding dimension, and hidden dimension."
      ],
      "metadata": {
        "id": "JZHITFSRYsXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import _______ # import pytorch\n",
        "import torch.nn as _______\n",
        "\n",
        "# Define model hyperparameters\n",
        "embed_size = 64   # dimensionality of character embeddings\n",
        "hidden_size = 128  # number of hidden units in RNN/LSTM/GRU\n",
        "\n",
        "# Reproducibility\n",
        "torch._______(42) # set a seed\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.embedding = nn._______(vocab_size, embed_dim)\n",
        "        self.rnn = nn._______(embed_dim, _______, batch_first=True)\n",
        "        self.fc = nn._______(hidden_dim, _______)\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch, seq_length) of character indices\n",
        "        x = self.embedding(x)                        # (batch, seq_length, embed_dim)\n",
        "        out, hidden = self.rnn(x, hidden)            # out: (batch, seq_length, hidden_dim)\n",
        "        out = self.fc(out)                           # (batch, seq_length, vocab_size)\n",
        "        return out, hidden\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.embedding = nn._______(vocab_size, embed_dim)\n",
        "        self.lstm = nn._______(embed_dim, hidden_dim, _______=True)\n",
        "        self.fc = nn._______(_______, vocab_size)\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)                        # (batch, seq_length, embed_dim)\n",
        "        out, hidden = self.lstm(x, hidden)           # out: (batch, seq_length, hidden_dim)\n",
        "        out = self.fc(out)                           # (batch, seq_length, vocab_size)\n",
        "        return out, hidden\n",
        "\n",
        "class CharGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, _______)\n",
        "        self.gru = nn._______(_______, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)                        # (batch, seq_length, embed_dim)\n",
        "        out, hidden = self._______(x, hidden)            # out: (batch, seq_length, hidden_dim)\n",
        "        out = self._______(out)                           # (batch, seq_length, vocab_size)\n",
        "        return out, hidden\n",
        "\n",
        "# Initialize one instance of each model\n",
        "model_rnn = CharRNN(_______, embed_size, hidden_size)\n",
        "model_lstm = CharLSTM(vocab_size, _______, hidden_size)\n",
        "model_gru = CharGRU(vocab_size, embed_size, _______)\n",
        "\n",
        "# Move models to device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch._______.is_available() else 'cpu')\n",
        "model_rnn._______(device)\n",
        "model_lstm.to(_______)\n",
        "model_gru.to(device)\n"
      ],
      "metadata": {
        "id": "EC3ZpobaUN8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have three models ready. Each model's `forward` returns the output for all time steps in the sequence, as well as the final hidden state (`hidden`). For the LSTM, `hidden` actually contains a tuple of `hidden_state, cell_state)`, but we won't need to manually use these in our training loop since we are resetting at each sequence."
      ],
      "metadata": {
        "id": "l_w0wkBWaAzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Configuration and Loop\n",
        "We'll train each model on the task of next-character prediction. Key points for training:\n",
        "- **Loss function:** We use cross-entropy loss between the predicted character distribution and the actual next character. PyTorch's nn.CrossEntropyLoss is suitable, as it expects class scores (logits) and true class indices.\n",
        "- **Optimizer:** We'll use Adam optimizer (a good default for RNNs) with the same learning rate for all models (e.g., 0.005).\n",
        "- **Epochs:** Train for 50 epochs as specified.\n",
        "Sequence length: 100 characters per training sequence.\n",
        "- **Batching:** We will use a batch size of 1 for simplicity (processing one sequence at a time). This means more weight updates per epoch, but given the text size and model, it should be manageable.\n",
        "\n",
        "- **Gradient clipping:** To prevent exploding gradients (which can happen with RNNs on long sequences), we'll apply gradient clipping (e.g., clip norm of gradients to 5) each time we backpropagate. This helps stabilize training for the vanilla RNN in particular.\n",
        "- **Hidden state reset:** We will reset the hidden state at the start of each sequence chunk. Since we treat each 100-char chunk independently, we start with a fresh hidden state (zero-initialized, which is the default if we don't pass a hidden state to the model).\n",
        "\n",
        "Now let's set up the loss, optimizers, and the training loop. We'll train each model in turn and record the average training loss per epoch for each."
      ],
      "metadata": {
        "id": "emlcSkbTaUt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "criterion = nn._______() # the loss function and the criteria is cross entropy loss (why?)\n",
        "\n",
        "# Optimizers for each model\n",
        "optimizer_rnn = torch._______.Adam(model_rnn.parameters(), lr=0.005)\n",
        "optimizer_lstm = torch.optim._______(model_lstm.parameters(), _______=0.005)\n",
        "optimizer_gru = _______.optim.Adam(model_gru._______(), lr=0.005)\n",
        "\n",
        "# Training parameters\n",
        "seq_length = 100\n",
        "num_epochs = 50\n",
        "\n",
        "# To store loss history\n",
        "loss_history = {\"RNN\": [], \"LSTM\": [], \"GRU\": []}\n",
        "\n",
        "# Training loop\n",
        "for model_name, model, optimizer in [(\"RNN\", model_rnn, optimizer_rnn),\n",
        "                                     (\"LSTM\", model_lstm, optimizer_lstm),\n",
        "                                     (\"GRU\", model_gru, optimizer_gru)]:\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    model.train()  # set model to training mode\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        # iterate through text in chunks of seq_length\n",
        "        for i in range(0, len(data_indices) - _______, seq_length): #why?\n",
        "            # Prepare input and target sequences\n",
        "            input_seq = data_indices[i : i+seq_length]\n",
        "            target_seq = data_indices[i+1 : i+seq_length+1]\n",
        "            # Convert to tensors and add batch dimension\n",
        "            inputs = torch._______(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "            targets = _______.tensor(target_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs)  # outputs: (batch=1, seq_length, vocab_size)\n",
        "            # Reshape outputs and targets to compute loss\n",
        "            outputs = outputs.reshape(-1, vocab_size)    # (seq_length, vocab_size)\n",
        "            targets = targets.reshape(-1)                # (seq_length,)\n",
        "            loss = criterion(outputs, targets)\n",
        "            # Backpropagation and optimization\n",
        "            optimizer._______()\n",
        "            loss.backward()\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "            total_loss += _______.item()\n",
        "            count += 1\n",
        "        avg_loss = total_loss / count\n",
        "        loss_history[model_name].append(avg_loss)\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch}/{num_epochs} — Training loss: {avg_loss:.4f}\")\n",
        "    print(f\"{model_name} training complete.\\n\")\n"
      ],
      "metadata": {
        "id": "7FHxnzJKUSIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this loop, for each model we go through 50 epochs. Within an epoch, we slide a window of length 100 through the text. At each step:\n",
        "- We take a 100-character slice as input_seq and the next 100 characters as target_seq.\n",
        "- We convert them to tensors and move to the chosen device (CPU or GPU).\n",
        "- We perform a forward pass through the model to get predictions for each position in the sequence.\n",
        "- We compute the cross-entropy loss between the predictions and the target characters.\n",
        "- We do a backward pass and update the model parameters using the optimizer.\n",
        "- We accumulate the loss to compute an average loss for the epoch.\n",
        "\n",
        "\n",
        "Notice we print the loss at epoch 1 and every 10 epochs to monitor progress. The gradient clipping ensures that if gradients explode (which can happen with plain RNN especially), we cap their norm to 5.0 to keep training stable.\n",
        "\n",
        "\n",
        "Also, because we start each sequence fresh, we do not carry over the hidden state from one chunk to the next. This means the model treats each 100-character segment independently. In practice, one could let the hidden state carry over between batches for stateful training, but that complicates training and isn't necessary for this comparison experiment."
      ],
      "metadata": {
        "id": "31bCrwQ5attn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Loss Visualization\n",
        "After training the models, we'll compare their performance by plotting the training loss across epochs for each model. This will give us a sense of:\n",
        "- How quickly each model learns (converges),\n",
        "- The final training loss each achieves (which indicates how well it fit the training data).\n",
        "\n",
        "Let's plot the loss curves for Vanilla RNN, LSTM, and GRU:"
      ],
      "metadata": {
        "id": "cal44Yaja3Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(loss_history[\"RNN\"], label=\"Vanilla RNN\")\n",
        "plt.plot(loss_history[\"LSTM\"], label=\"LSTM\")\n",
        "plt.plot(loss_history[\"GRU\"], label=\"GRU\")\n",
        "plt.title(\"Training Loss over Epochs for RNN, LSTM, and GRU\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lU93sldzUVi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Text Generation with the Trained Models\n",
        "With our models trained and their performance compared via training loss, the next step is to see how well they can generate poetry in a character-level manner. In this section, we implement and use a generic text generator function that works with any of our three architectures (Vanilla RNN, LSTM, or GRU).\n",
        "\n",
        "### How the Generator Works\n",
        "1. **Seed Initialization:**\n",
        "The generator starts with a given seed text (for example, \"The nightingale \"). This seed is converted to a sequence of character indices using our vocabulary mapping. The model then processes these indices to build an initial hidden state.\n",
        "\n",
        "2. **Iterative Generation:**\n",
        "Once the initial state is built, the generator repeatedly predicts one character at a time:\n",
        "\n",
        "- At each step, it takes the most recently generated character as input.\n",
        "\n",
        "- It computes the next-character probabilities by outputting logits (raw scores) for all possible characters.\n",
        "\n",
        "- Temperature scaling is applied to these logits. The temperature controls randomness:\n",
        "\n",
        "    - A lower temperature (< 1) produces more conservative and predictable text.\n",
        "\n",
        "    - A higher temperature (> 1) creates more diverse and unexpected outputs.\n",
        "\n",
        "- A new character is sampled from the probability distribution and appended to the generated text.\n",
        "\n",
        "- The process repeats for a specified number of iterations, generating the desired length of text.\n",
        "\n",
        "3. **Model-Agnostic Function:**\n",
        "The generator function is designed to work identically with any of our models. Whether it's the Vanilla RNN, the LSTM, or the GRU, the function uses the model's forward pass to update the hidden state and predict the next character.\n",
        "\n",
        "4. **Output Comparison:**\n",
        "Finally, the generated text from each model is printed. This qualitative evaluation lets us observe differences in language quality, coherence, and style, which are influenced by how well each architecture captures long-term dependencies."
      ],
      "metadata": {
        "id": "DzslVrbUbWkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, gen_length=200, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate text using a trained character-level model.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The trained PyTorch model (RNN, LSTM, or GRU).\n",
        "        start_text (str): The seed text to start generation.\n",
        "        gen_length (int): Number of characters to generate.\n",
        "        temperature (float): Sampling temperature controlling randomness.\n",
        "\n",
        "    Returns:\n",
        "        generated_text (str): The full generated text.\n",
        "    \"\"\"\n",
        "    model._______()  # Switch to evaluation mode\n",
        "    # Convert the seed text to indices using the global mapping `char_to_idx`\n",
        "    input_seq = [char_to_idx[ch] for ch in start_text]\n",
        "    input_tensor = torch.tensor(input_seq, dtype=torch.long)._______(0).to(device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden = None\n",
        "    generated_text = start_text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Run the seed text to build up the hidden state\n",
        "        _, hidden = model(input_tensor, hidden)\n",
        "        # Extract the last character index (as a scalar) from the seed text\n",
        "        last_char = input_tensor[:, -1].item()\n",
        "\n",
        "        for _ in range(gen_length):\n",
        "            # Create input tensor from the last character; shape: (1, 1)\n",
        "            inp = torch.tensor([[last_char]], dtype=torch.long).to(device)\n",
        "            output, hidden = model(inp, hidden)\n",
        "            # Remove extra dimensions to get (vocab_size,)\n",
        "            output = output._______(0)._______(0)\n",
        "            # Apply temperature scaling to adjust randomness in sampling\n",
        "            output_div = output._______(temperature).exp()\n",
        "            probs = output_div / output_div.sum()\n",
        "            # Sample the next character index\n",
        "            next_idx = torch._______(probs, 1).item()\n",
        "            next_char = idx_to_char[next_idx]\n",
        "            generated_text += next_char\n",
        "            # Update last_char as a scalar for the next iteration\n",
        "            last_char = next_idx\n",
        "    return generated_text\n",
        "\n",
        "# Test the generator for each model with a seed text.\n",
        "seed_text = \"The nightingale \"\n",
        "gen_length = 300\n",
        "\n",
        "print(\"Generated text using Vanilla RNN:\")\n",
        "print(generate_text(model_rnn, seed_text, gen_length, temperature=0.8))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"Generated text using LSTM:\")\n",
        "print(generate_text(model_lstm, seed_text, gen_length, temperature=0.8))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"Generated text using GRU:\")\n",
        "print(generate_text(model_gru, seed_text, gen_length, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "gdHtMuXGVckr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}