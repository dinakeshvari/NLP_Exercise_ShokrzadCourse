{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinakeshvari/NLP_Exercise_ShokrzadCourse/blob/main/Copy_of_Assignment01_DS04_S01_NLTK_SpaCy_RezaShokrzad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b057fad5",
      "metadata": {
        "id": "b057fad5"
      },
      "source": [
        "# **Practice Assignment: NLP with NLTK & spaCy**\n",
        "\n",
        "* This assignment is part of the NLP Workshop on YouTube, which is free and open to the public.\n",
        "* **Lecturer: Reza Shokrzad.**\n",
        "*‚Äå [ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿ¨ŸÑÿ≥Ÿá ÿßŸàŸÑ ⁄©ŸÑÿßÿ≥](https://youtube.com/live/lDCoqQSc4ZE?feature=share)\n",
        "* [ÿ®ÿ±ŸÜÿßŸÖŸá ÿßÿ¨ÿ±ÿß€å€å ⁄©ŸÑÿßÿ≥ Ÿà ÿ¨ŸÑÿ≥ÿßÿ™](https://docs.google.com/spreadsheets/d/1SP3NJ9H7yp8sgof-zp_t4oxmdxjMdEgoL_mmCDvdUm4/edit?gid=0#gid=0)\n",
        "\n",
        "\n",
        "Welcome to this **Fill-in-the-Blanks NLP Assignment!** üéØ This exercise will help you solidify your understanding of **NLTK** and **spaCy** by filling in the missing parts of the code. Follow the instructions carefully, and make sure to test your solutions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa085b7f",
      "metadata": {
        "id": "aa085b7f"
      },
      "source": [
        "## **1. Working with Corpora & Lexical Resources**\n",
        "**Task:** Load and analyze texts from different corpora.\n",
        "- Use NLTK‚Äôs **Gutenberg** corpus to load the text of *Moby Dick*.\n",
        "- Tokenize it into words.\n",
        "- Count the top 10 most frequent words (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.fileids()"
      ],
      "metadata": {
        "id": "6H9eTy7X0c0W",
        "outputId": "3e618769-8371-4bef-d088-a0ab6cb68d2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6H9eTy7X0c0W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "757d9099",
      "metadata": {
        "id": "757d9099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26294e07-0f2a-43fa-c7e0-75ed66216272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('whale', 1095), ('one', 913), ('like', 580), ('upon', 565), ('ahab', 511), ('man', 498), ('ship', 469), ('old', 443), ('ye', 438), ('would', 436)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "# Load text\n",
        "file_id = \"melville-moby_dick.txt\"\n",
        "text = gutenberg.raw(file_id)\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]  # FILL THIS\n",
        "\n",
        "# Compute frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "# Print top 10 words\n",
        "print(fdist.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2894d7a",
      "metadata": {
        "id": "f2894d7a"
      },
      "source": [
        "## **2. Tokenization Techniques**\n",
        "**Task:** Tokenize a given text using both **NLTK** and **spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a9ac50",
      "metadata": {
        "id": "85a9ac50"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from nltk.__________ import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.__________('en_core_web_sm')\n",
        "\n",
        "text = \"SpaCy is fast! However, NLTK provides flexibility in tokenization.\"\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_word_tokens = __________(text)  # FILL THIS\n",
        "nltk_sent_tokens = __________(text)  # FILL THIS\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [__________.text for token in doc]\n",
        "\n",
        "print(\"NLTK Word Tokens:\", nltk_word_tokens)\n",
        "print(\"NLTK Sentence Tokens:\", nltk_sent_tokens)\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Regex Pattern Matching for Phone Number Detection**\n",
        "**Task:** Write a pattern using regex to find the phone nymber in the text."
      ],
      "metadata": {
        "id": "Sp7RCE4fIaVV"
      },
      "id": "Sp7RCE4fIaVV"
    },
    {
      "cell_type": "code",
      "source": [
        "import __________\n",
        "\n",
        "# Example 2: Phone Number Extraction\n",
        "text_phones = \"Call me at +1-202-555-0173 or reach our office at (415) 123-4567.\"\n",
        "phone_pattern = r\"__________\"  # Regex for phone numbers\n",
        "\n",
        "phones = re.findall(__________, __________)\n",
        "print(\"Detected Phone Numbers:\", phones)\n"
      ],
      "metadata": {
        "id": "XeSzDZ2-Icdv"
      },
      "id": "XeSzDZ2-Icdv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Stopwords Filtering using NLTK**\n",
        "**Task:** Analyze movie reviews where stopwords are removed to focus on meaningful words."
      ],
      "metadata": {
        "id": "pdn94C-1cGgc"
      },
      "id": "pdn94C-1cGgc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.__________ import stopwords\n",
        "from nltk.__________ import word_tokenize\n",
        "\n",
        "# üé¨ Sample Movie Review\n",
        "review = \"\"\"The movie was absolutely amazing! The cinematography was stunning, and the characters were incredibly well-developed.\n",
        "However, the storyline felt a bit predictable at times, and some scenes were unnecessarily long. Overall, a great experience!\"\"\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(review)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.__________() not in __________.words(\"english\") and word.__________()]\n",
        "\n",
        "# Output results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nFiltered (No Stopwords):\", filtered_words)\n"
      ],
      "metadata": {
        "id": "qFGnnFOjcM23"
      },
      "id": "qFGnnFOjcM23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Stemming Methods using NLTK**\n",
        "**Task:** Analyze legal and scientific terms to observe how different stemming algorithms behave."
      ],
      "metadata": {
        "id": "7UezErpHcu3x"
      },
      "id": "7UezErpHcu3x"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.__________ import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# ‚öñÔ∏è Sample Legal & Scientific Terms\n",
        "words = [\"arguing\", \"justification\", \"liable\", \"obligations\", \"classification\", \"microbiology\", \"evolutionary\", \"running\", \"happiness\"]\n",
        "\n",
        "# Initialize Stemmer Objects\n",
        "porter = __________()\n",
        "lancaster = __________()\n",
        "\n",
        "# Apply Stemming\n",
        "porter_stems = [porter.stem(word) for word in __________]\n",
        "lancaster_stems = [lancaster.__________(word) for word in words]\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nPorter Stemmer Results:\", porter_stems)\n",
        "print(\"\\nLancaster Stemmer Results:\", lancaster_stems)\n"
      ],
      "metadata": {
        "id": "Kpvzkd31d6Na"
      },
      "id": "Kpvzkd31d6Na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **Lemmatization Strategies using NLTK & spaCy**\n",
        "\n",
        "### NLTK‚Äôs WordNetLemmatizer\n",
        "**Task:** Lemmatize a political news headline to show how lemmatization helps retain the correct part of speech (POS) while normalizing words."
      ],
      "metadata": {
        "id": "ffIhEdRJeoI4"
      },
      "id": "ffIhEdRJeoI4"
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.__________ import WordNetLemmatizer\n",
        "from nltk.__________ import wordnet\n",
        "from nltk.__________ import word_tokenize\n",
        "\n",
        "# üì∞ Sample News Headline\n",
        "headline = \"The senators debated the increasing regulations affecting technology companies.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(__________)\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = __________()\n",
        "\n",
        "# Apply Lemmatization (default without POS tagging)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for __________ in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "o4LlZqSxe086"
      },
      "id": "o4LlZqSxe086",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy‚Äôs Built-in Lemmatizer"
      ],
      "metadata": {
        "id": "C61P_pdvfJlH"
      },
      "id": "C61P_pdvfJlH"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = __________.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the same headline\n",
        "doc = __________(headline)\n",
        "\n",
        "# Apply Lemmatization\n",
        "spacy_lemmatized = [token.__________ for token in doc]\n",
        "\n",
        "print(\"\\nspaCy Lemmatized Words:\", spacy_lemmatized)\n"
      ],
      "metadata": {
        "id": "4qPNWbqafWt3"
      },
      "id": "4qPNWbqafWt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **Parsing & Chunking using NLTK**\n",
        "\n",
        "**Task:** Analyze legal contracts and job descriptions where parsing and chunking help extract meaningful phrases like noun phrases (NPs) or verb phrases (VPs)."
      ],
      "metadata": {
        "id": "nf-YB5yjfdl5"
      },
      "id": "nf-YB5yjfdl5"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìú Task: Extracting Key Phrases from Legal & Job Documents\n",
        "import nltk\n",
        "\n",
        "# nltk.download(\"punkt\")\n",
        "nltk.download(\"__________\")\n",
        "\n",
        "# üìú Sample Legal Contract Text\n",
        "contract_text = \"The tenant shall pay the monthly rent before the 5th of each month.\"\n",
        "\n",
        "# Tokenize & POS Tagging\n",
        "words = __________.word_tokenize(contract_text)\n",
        "pos_tags = nltk.__________(words)\n",
        "\n",
        "# Define a Chunking Grammar for Noun Phrases (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "# Apply Chunking\n",
        "chunk_parser = nltk.__________(grammar)\n",
        "tree = chunk_parser.__________(pos_tags)\n",
        "\n",
        "# Display Results\n",
        "print(\"Chunked Tree:\")\n",
        "tree.pretty_print()\n"
      ],
      "metadata": {
        "id": "t9e4fOIuft4n"
      },
      "id": "t9e4fOIuft4n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **Exploring Hyponyms & Hypernyms using WordNet (NLTK)**\n",
        "\n",
        "**Task:** Hyponyms (specific terms) and hypernyms (general terms) in scientific and business domains, where hierarchical relationships between words are essential."
      ],
      "metadata": {
        "id": "GTmtGRjofqpB"
      },
      "id": "GTmtGRjofqpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Task: Explore Word Relationships in Science & Business\n",
        "from nltk.__________ import wordnet\n",
        "\n",
        "# ü¶Å Find Hypernyms & Hyponyms for \"lion\"\n",
        "word = \"lion\"\n",
        "synset = __________.synsets(word)[0]  # Selecting the first synset\n",
        "\n",
        "# Hypernyms (More General Category)\n",
        "hypernyms = synset.__________()\n",
        "print(f\"Hypernyms (More General Concept) of '{word}':\")\n",
        "print([hypernym.name().__________('.')[0] for hypernym in hypernyms])\n",
        "\n",
        "# Hyponyms (More Specific Types)\n",
        "hyponyms = __________.hyponyms()\n",
        "print(f\"\\nHyponyms (More Specific Types) of '{word}':\")\n",
        "print([hyponym.__________().split('.')[0] for hyponym in hyponyms])\n"
      ],
      "metadata": {
        "id": "EELbOY0dgjsK"
      },
      "id": "EELbOY0dgjsK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fdb5a06b",
      "metadata": {
        "id": "fdb5a06b"
      },
      "source": [
        "## **9. Named Entity Recognition (NER) with spaCy**\n",
        "**Task:** Extract named entities from a complex sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7654be",
      "metadata": {
        "id": "dc7654be"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\"\n",
        "\n",
        "doc = nlp(__________)  # FILL THIS\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}